{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = \"C:\\chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = 'C:\\chromedriver.exe')\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get('chrome://dino')\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[0] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if actions[0] == 1:\n",
    "            self._agent.duck()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"\\t STATE\", state, \"\\n EPSILON\", epsilon, \"\\t ACTION\", action_index, \"\\n REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rayyan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "fps: 2.223589760989378\n",
      "TIMESTEP 1 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.268594656654358\n",
      "TIMESTEP 2 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13907689939781\n",
      "TIMESTEP 3 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 12.665988216713423\n",
      "TIMESTEP 4 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 10.644793210565854\n",
      "TIMESTEP 5 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.394200983630626\n",
      "TIMESTEP 6 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 15.882521782924307\n",
      "TIMESTEP 7 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 13.52198694968148\n",
      "TIMESTEP 8 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.994996328575457\n",
      "TIMESTEP 9 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.82838311082565\n",
      "TIMESTEP 10 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.66606471485301\n",
      "TIMESTEP 11 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138890599027274\n",
      "TIMESTEP 12 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139139000477137\n",
      "TIMESTEP 13 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138952698672885\n",
      "TIMESTEP 14 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403479131468618\n",
      "TIMESTEP 15 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.138952698672885\n",
      "TIMESTEP 16 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882942789197049\n",
      "TIMESTEP 17 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634345354580189\n",
      "TIMESTEP 18 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.677219392522435\n",
      "TIMESTEP 19 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882702211451075\n",
      "TIMESTEP 20 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138952698672885\n",
      "TIMESTEP 21 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.676887842738086\n",
      "TIMESTEP 22 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.994956066971532\n",
      "TIMESTEP 23 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882702211451075\n",
      "TIMESTEP 24 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.403414979448332\n",
      "TIMESTEP 25 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 15.274843493366449\n",
      "TIMESTEP 26 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 13.166118379749378\n",
      "TIMESTEP 27 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139014798796396\n",
      "TIMESTEP 28 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13907689939781\n",
      "TIMESTEP 29 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138828499859557\n",
      "TIMESTEP 30 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138828499859557\n",
      "TIMESTEP 31 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.67702046106989\n",
      "TIMESTEP 32 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.872664590326474\n",
      "TIMESTEP 33 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.124124462760353\n",
      "TIMESTEP 34 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.387264650379567\n",
      "TIMESTEP 35 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.12629425854797\n",
      "TIMESTEP 36 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.121149692129112\n",
      "TIMESTEP 37 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.393477479167643\n",
      "TIMESTEP 38 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.86858104912699\n",
      "TIMESTEP 39 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.125302279446537\n",
      "TIMESTEP 40 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.86708027540289\n",
      "TIMESTEP 41 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.662378884726465\n",
      "TIMESTEP 42 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.878072509908879\n",
      "TIMESTEP 43 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.129953236524734\n",
      "TIMESTEP 44 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.129332953907447\n",
      "TIMESTEP 45 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.388929482695968\n",
      "TIMESTEP 46 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.129270928269555\n",
      "TIMESTEP 47 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13528963707847\n",
      "TIMESTEP 48 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882762355204314\n",
      "TIMESTEP 49 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403350827929824\n",
      "TIMESTEP 50 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13224869709033\n",
      "TIMESTEP 51 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403607437014557\n",
      "TIMESTEP 52 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634403632107473\n",
      "TIMESTEP 53 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.403735744567683\n",
      "TIMESTEP 54 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403607437014557\n",
      "TIMESTEP 55 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138890599027274\n",
      "TIMESTEP 56 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882581925311078\n",
      "TIMESTEP 57 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139201102034377\n",
      "TIMESTEP 58 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138890599027274\n",
      "TIMESTEP 59 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.6346367465613\n",
      "TIMESTEP 60 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882642068153332\n",
      "TIMESTEP 61 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139201102034377\n",
      "TIMESTEP 62 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403414979448332\n",
      "TIMESTEP 63 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634520188465437\n",
      "TIMESTEP 64 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138890599027274\n",
      "TIMESTEP 65 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.123876523276824\n",
      "TIMESTEP 66 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.128960807235586\n",
      "TIMESTEP 67 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.138828499859557\n",
      "TIMESTEP 68 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882642068153332\n",
      "TIMESTEP 69 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139139000477137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 70 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882702211451075\n",
      "TIMESTEP 71 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 13.165911737655106\n",
      "TIMESTEP 72 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634520188465437\n",
      "TIMESTEP 73 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403479131468618\n",
      "TIMESTEP 74 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13907689939781\n",
      "TIMESTEP 75 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139014798796396\n",
      "TIMESTEP 76 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634287077487365\n",
      "TIMESTEP 77 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403799899096956\n",
      "TIMESTEP 78 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403607437014557\n",
      "TIMESTEP 79 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.6346367465613\n",
      "TIMESTEP 80 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.714790906539433\n",
      "TIMESTEP 81 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD -1 / Q_MAX  0 / Loss  0\n",
      "fps: 13.707054994182931\n",
      "TIMESTEP 82 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.677086771026755\n",
      "TIMESTEP 83 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882581925311078\n",
      "TIMESTEP 84 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139014798796396\n",
      "TIMESTEP 85 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.139014798796396\n",
      "TIMESTEP 86 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.882642068153332\n",
      "TIMESTEP 87 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403479131468618\n",
      "TIMESTEP 88 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403607437014557\n",
      "TIMESTEP 89 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 15.882882644077295\n",
      "TIMESTEP 90 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.138952698672885\n",
      "TIMESTEP 91 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.134172423874073\n",
      "TIMESTEP 92 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.625957923991967\n",
      "TIMESTEP 93 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403799899096956\n",
      "TIMESTEP 94 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.132807151154292\n",
      "TIMESTEP 95 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.13454481108487\n",
      "TIMESTEP 96 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634403632107473\n",
      "TIMESTEP 97 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.959428095457596\n",
      "TIMESTEP 98 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.634695026260955\n",
      "TIMESTEP 99 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403607437014557\n",
      "TIMESTEP 100 \t STATE observe \n",
      " EPSILON 0.1 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403479131468618\n",
      "TIMESTEP 101 \t STATE explore \n",
      " EPSILON 0.1 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.403864054128046\n",
      "WARNING:tensorflow:From C:\\Users\\Rayyan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "TIMESTEP 102 \t STATE explore \n",
      " EPSILON 0.099999001 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.955287 / Loss  0.6633365154266357\n",
      "fps: 0.8002582620581441\n",
      "TIMESTEP 103 \t STATE explore \n",
      " EPSILON 0.099998002 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  14.460461 / Loss  1.5850619077682495\n",
      "fps: 7.046597583430776\n",
      "TIMESTEP 104 \t STATE explore \n",
      " EPSILON 0.099997003 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.883863 / Loss  0.6187434196472168\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 105 \t STATE explore \n",
      " EPSILON 0.099996004 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.301869 / Loss  0.7932872772216797\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 106 \t STATE explore \n",
      " EPSILON 0.099995005 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  16.567215 / Loss  2.3907127380371094\n",
      "fps: 7.198717574650817\n",
      "TIMESTEP 107 \t STATE explore \n",
      " EPSILON 0.099994006 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.148758 / Loss  9.160772323608398\n",
      "fps: 7.198717574650817\n",
      "TIMESTEP 108 \t STATE explore \n",
      " EPSILON 0.099993007 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.607392 / Loss  8.37224292755127\n",
      "fps: 7.096528807043903\n",
      "TIMESTEP 109 \t STATE explore \n",
      " EPSILON 0.099992008 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.77188 / Loss  0.6437444686889648\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 110 \t STATE explore \n",
      " EPSILON 0.09999100899999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.19217 / Loss  0.29411783814430237\n",
      "fps: 7.198655799097573\n",
      "TIMESTEP 111 \t STATE explore \n",
      " EPSILON 0.09999000999999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.666871 / Loss  0.35476770997047424\n",
      "----------Random Action----------\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 112 \t STATE explore \n",
      " EPSILON 0.09998901099999999 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.835857 / Loss  5.7723164558410645\n",
      "fps: 7.046550229491892\n",
      "TIMESTEP 113 \t STATE explore \n",
      " EPSILON 0.09998801199999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.77489 / Loss  0.4734352231025696\n",
      "fps: 7.198717574650817\n",
      "TIMESTEP 114 \t STATE explore \n",
      " EPSILON 0.09998701299999999 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.685976 / Loss  4.627538204193115\n",
      "fps: 7.19868050919164\n",
      "TIMESTEP 115 \t STATE explore \n",
      " EPSILON 0.09998601399999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.627567 / Loss  0.6448500156402588\n",
      "fps: 7.147294483315668\n",
      "TIMESTEP 116 \t STATE explore \n",
      " EPSILON 0.09998501499999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.095643 / Loss  0.5593906044960022\n",
      "fps: 7.147306662667828\n",
      "TIMESTEP 117 \t STATE explore \n",
      " EPSILON 0.09998401599999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.212498 / Loss  0.4518527686595917\n",
      "fps: 7.096540813995787\n",
      "TIMESTEP 118 \t STATE explore \n",
      " EPSILON 0.09998301699999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.427329 / Loss  0.5525774955749512\n",
      "fps: 7.198643444114154\n",
      "TIMESTEP 119 \t STATE explore \n",
      " EPSILON 0.09998201799999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.706754 / Loss  0.3525451123714447\n",
      "fps: 7.046597583430776\n",
      "TIMESTEP 120 \t STATE explore \n",
      " EPSILON 0.09998101899999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.586871 / Loss  0.2191469967365265\n",
      "fps: 7.198643444114154\n",
      "TIMESTEP 121 \t STATE explore \n",
      " EPSILON 0.09998001999999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.909541 / Loss  3.279026985168457\n",
      "fps: 6.414191555412825\n",
      "TIMESTEP 122 \t STATE explore \n",
      " EPSILON 0.09997902099999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.85519 / Loss  0.26128798723220825\n",
      "fps: 7.046621260638887\n",
      "TIMESTEP 123 \t STATE explore \n",
      " EPSILON 0.09997802199999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.618226 / Loss  0.6387673616409302\n",
      "fps: 7.250835842878826\n",
      "TIMESTEP 124 \t STATE explore \n",
      " EPSILON 0.09997702299999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.180226 / Loss  0.9331377744674683\n",
      "fps: 7.046573906381778\n",
      "TIMESTEP 125 \t STATE explore \n",
      " EPSILON 0.09997602399999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.618143 / Loss  0.4761437773704529\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 126 \t STATE explore \n",
      " EPSILON 0.09997502499999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.985904 / Loss  0.5891708135604858\n",
      "fps: 7.096540813995787\n",
      "TIMESTEP 127 \t STATE explore \n",
      " EPSILON 0.09997402599999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.675006 / Loss  0.4840015769004822\n",
      "----------Random Action----------\n",
      "fps: 7.147184871014081\n",
      "TIMESTEP 128 \t STATE explore \n",
      " EPSILON 0.09997302699999996 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  9.978348 / Loss  0.5489948987960815\n",
      "fps: 6.451157632778242\n",
      "TIMESTEP 129 \t STATE explore \n",
      " EPSILON 0.09997202799999996 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.812662 / Loss  0.6482577919960022\n",
      "----------Random Action----------\n",
      "fps: 7.085834763687475\n",
      "TIMESTEP 130 \t STATE explore \n",
      " EPSILON 0.09997102899999996 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  9.055753 / Loss  0.2503712773323059\n",
      "fps: 7.137515698333339\n",
      "TIMESTEP 131 \t STATE explore \n",
      " EPSILON 0.09997002999999996 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  14.087467 / Loss  0.5211248397827148\n",
      "fps: 7.085715057995939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 132 \t STATE explore \n",
      " EPSILON 0.09996903099999996 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.524648 / Loss  0.5036306977272034\n",
      "fps: 7.239572078058817\n",
      "TIMESTEP 133 \t STATE explore \n",
      " EPSILON 0.09996803199999996 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.838965 / Loss  0.19858069717884064\n",
      "fps: 7.096528807043903\n",
      "TIMESTEP 134 \t STATE explore \n",
      " EPSILON 0.09996703299999996 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.589549 / Loss  0.25393497943878174\n",
      "fps: 7.096600849364669\n",
      "TIMESTEP 135 \t STATE explore \n",
      " EPSILON 0.09996603399999995 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.258528 / Loss  0.7029576301574707\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 136 \t STATE explore \n",
      " EPSILON 0.09996503499999995 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.821321 / Loss  0.36910587549209595\n",
      "fps: 7.198668154123402\n",
      "TIMESTEP 137 \t STATE explore \n",
      " EPSILON 0.09996403599999995 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.869523 / Loss  0.32822003960609436\n",
      "fps: 7.19868050919164\n",
      "TIMESTEP 138 \t STATE explore \n",
      " EPSILON 0.09996303699999995 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.027598 / Loss  0.5271520614624023\n",
      "fps: 7.046562067916946\n",
      "TIMESTEP 139 \t STATE explore \n",
      " EPSILON 0.09996203799999995 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.261117 / Loss  0.27452486753463745\n",
      "fps: 7.2508483776667925\n",
      "TIMESTEP 140 \t STATE explore \n",
      " EPSILON 0.09996103899999995 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.017261 / Loss  0.2817348837852478\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 141 \t STATE explore \n",
      " EPSILON 0.09996003999999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.019136 / Loss  0.21648025512695312\n",
      "fps: 7.147221408074378\n",
      "TIMESTEP 142 \t STATE explore \n",
      " EPSILON 0.09995904099999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.096175 / Loss  0.42707324028015137\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 143 \t STATE explore \n",
      " EPSILON 0.09995804199999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.251979 / Loss  0.5501779317855835\n",
      "fps: 7.198668154123402\n",
      "TIMESTEP 144 \t STATE explore \n",
      " EPSILON 0.09995704299999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.528533 / Loss  0.5505733489990234\n",
      "fps: 7.147282304005016\n",
      "TIMESTEP 145 \t STATE explore \n",
      " EPSILON 0.09995604399999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.203491 / Loss  0.17398519814014435\n",
      "----------Random Action----------\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 146 \t STATE explore \n",
      " EPSILON 0.09995504499999994 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.494527 / Loss  2.7051613330841064\n",
      "fps: 7.147282304005016\n",
      "TIMESTEP 147 \t STATE explore \n",
      " EPSILON 0.09995404599999994 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.7818 / Loss  0.894778847694397\n",
      "----------Random Action----------\n",
      "fps: 6.900750899961172\n",
      "TIMESTEP 148 \t STATE explore \n",
      " EPSILON 0.09995304699999993 \t ACTION 1 \n",
      " REWARD -1 / Q_MAX  13.724474 / Loss  0.34536483883857727\n",
      "----------Random Action----------\n",
      "fps: 6.670765746494284\n",
      "TIMESTEP 149 \t STATE explore \n",
      " EPSILON 0.09995204799999993 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.04467 / Loss  0.42790043354034424\n",
      "fps: 7.045496768948309\n",
      "TIMESTEP 150 \t STATE explore \n",
      " EPSILON 0.09995104899999993 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.557895 / Loss  0.6251657009124756\n",
      "fps: 7.133958685909157\n",
      "TIMESTEP 151 \t STATE explore \n",
      " EPSILON 0.09995004999999993 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.049385 / Loss  2.8955209255218506\n",
      "fps: 7.091573407005507\n",
      "TIMESTEP 152 \t STATE explore \n",
      " EPSILON 0.09994905099999993 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  7.9981513 / Loss  0.36219102144241333\n",
      "fps: 7.249845731560816\n",
      "TIMESTEP 153 \t STATE explore \n",
      " EPSILON 0.09994805199999993 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.040302 / Loss  0.3817402124404907\n",
      "fps: 7.145930658488798\n",
      "TIMESTEP 154 \t STATE explore \n",
      " EPSILON 0.09994705299999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.969142 / Loss  0.32516855001449585\n",
      "fps: 6.370672122507317\n",
      "TIMESTEP 155 \t STATE explore \n",
      " EPSILON 0.09994605399999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.975707 / Loss  0.6234933137893677\n",
      "----------Random Action----------\n",
      "fps: 7.095340319893087\n",
      "TIMESTEP 156 \t STATE explore \n",
      " EPSILON 0.09994505499999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.7019825 / Loss  0.8027496933937073\n",
      "fps: 7.142632841809132\n",
      "TIMESTEP 157 \t STATE explore \n",
      " EPSILON 0.09994405599999992 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  9.652902 / Loss  0.24632319808006287\n",
      "fps: 6.453698759976428\n",
      "TIMESTEP 158 \t STATE explore \n",
      " EPSILON 0.09994305699999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.634164 / Loss  2.0097365379333496\n",
      "fps: 7.197148804079805\n",
      "TIMESTEP 159 \t STATE explore \n",
      " EPSILON 0.09994205799999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.528424 / Loss  0.20593079924583435\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 160 \t STATE explore \n",
      " EPSILON 0.09994105899999992 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.246246 / Loss  0.1539948284626007\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 161 \t STATE explore \n",
      " EPSILON 0.09994005999999991 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  7.7156515 / Loss  0.26525580883026123\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 162 \t STATE explore \n",
      " EPSILON 0.09993906099999991 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.190169 / Loss  1.794769525527954\n",
      "fps: 7.147270124735874\n",
      "TIMESTEP 163 \t STATE explore \n",
      " EPSILON 0.09993806199999991 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.042146 / Loss  0.264823853969574\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 164 \t STATE explore \n",
      " EPSILON 0.09993706299999991 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.381063 / Loss  0.9053683280944824\n",
      "fps: 7.198705219455348\n",
      "TIMESTEP 165 \t STATE explore \n",
      " EPSILON 0.09993606399999991 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.197412 / Loss  0.19776800274848938\n",
      "fps: 6.9972356749741005\n",
      "TIMESTEP 166 \t STATE explore \n",
      " EPSILON 0.0999350649999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.951069 / Loss  0.17204299569129944\n",
      "fps: 7.198754640491691\n",
      "TIMESTEP 167 \t STATE explore \n",
      " EPSILON 0.0999340659999999 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  6.746845 / Loss  0.09263936430215836\n",
      "fps: 6.455566175531268\n",
      "TIMESTEP 168 \t STATE explore \n",
      " EPSILON 0.0999330669999999 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.458149 / Loss  0.16566702723503113\n",
      "fps: 7.147221408074378\n",
      "TIMESTEP 169 \t STATE explore \n",
      " EPSILON 0.0999320679999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.154933 / Loss  0.2185162901878357\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 170 \t STATE explore \n",
      " EPSILON 0.0999310689999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.532784 / Loss  1.2671239376068115\n",
      "fps: 7.147221408074378\n",
      "TIMESTEP 171 \t STATE explore \n",
      " EPSILON 0.0999300699999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.635127 / Loss  0.12049710005521774\n",
      "----------Random Action----------\n",
      "fps: 7.092832598284586\n",
      "TIMESTEP 172 \t STATE explore \n",
      " EPSILON 0.0999290709999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.691811 / Loss  0.41258740425109863\n",
      "----------Random Action----------\n",
      "fps: 7.089487731207204\n",
      "TIMESTEP 173 \t STATE explore \n",
      " EPSILON 0.0999280719999999 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.937533 / Loss  0.10351363569498062\n",
      "fps: 7.18810774867782\n",
      "TIMESTEP 174 \t STATE explore \n",
      " EPSILON 0.0999270729999999 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  7.520768 / Loss  0.18504078686237335\n",
      "----------Random Action----------\n",
      "fps: 7.1404197799466465\n",
      "TIMESTEP 175 \t STATE explore \n",
      " EPSILON 0.09992607399999989 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.874493 / Loss  0.3052685260772705\n",
      "fps: 7.133352041293571\n",
      "TIMESTEP 176 \t STATE explore \n",
      " EPSILON 0.09992507499999989 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.239539 / Loss  0.4489172399044037\n",
      "fps: 7.146442031800555\n",
      "TIMESTEP 177 \t STATE explore \n",
      " EPSILON 0.09992407599999989 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  6.174492 / Loss  0.4272955656051636\n",
      "fps: 7.090170834699763\n",
      "TIMESTEP 178 \t STATE explore \n",
      " EPSILON 0.09992307699999989 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.864693 / Loss  0.4496825933456421\n",
      "----------Random Action----------\n",
      "fps: 6.4110444170331276\n",
      "TIMESTEP 179 \t STATE explore \n",
      " EPSILON 0.09992207799999989 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.631199 / Loss  0.37312355637550354\n",
      "fps: 7.196543713174863\n",
      "TIMESTEP 180 \t STATE explore \n",
      " EPSILON 0.09992107899999988 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.457944 / Loss  0.28487610816955566\n",
      "fps: 7.146222862848127\n",
      "TIMESTEP 181 \t STATE explore \n",
      " EPSILON 0.09992007999999988 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.289867 / Loss  0.2325310856103897\n",
      "fps: 7.094608217917577\n",
      "TIMESTEP 182 \t STATE explore \n",
      " EPSILON 0.09991908099999988 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.343035 / Loss  0.43864771723747253\n",
      "fps: 7.144981159331412\n",
      "TIMESTEP 183 \t STATE explore \n",
      " EPSILON 0.09991808199999988 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.337577 / Loss  0.26094892621040344\n",
      "fps: 7.095388331858759\n",
      "TIMESTEP 184 \t STATE explore \n",
      " EPSILON 0.09991708299999988 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  14.945961 / Loss  0.19813638925552368\n",
      "fps: 7.096552820988302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 185 \t STATE explore \n",
      " EPSILON 0.09991608399999988 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.771335 / Loss  0.31287989020347595\n",
      "fps: 7.143046423030483\n",
      "TIMESTEP 186 \t STATE explore \n",
      " EPSILON 0.09991508499999988 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.631937 / Loss  0.36968743801116943\n",
      "fps: 7.094788228985708\n",
      "TIMESTEP 187 \t STATE explore \n",
      " EPSILON 0.09991408599999987 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.205051 / Loss  0.2861787676811218\n",
      "fps: 7.19629676653364\n",
      "TIMESTEP 188 \t STATE explore \n",
      " EPSILON 0.09991308699999987 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.27859 / Loss  0.25170016288757324\n",
      "fps: 7.197556371623708\n",
      "TIMESTEP 189 \t STATE explore \n",
      " EPSILON 0.09991208799999987 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.095083 / Loss  0.20290139317512512\n",
      "fps: 7.046573906381778\n",
      "TIMESTEP 190 \t STATE explore \n",
      " EPSILON 0.09991108899999987 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.286111 / Loss  0.13774585723876953\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 191 \t STATE explore \n",
      " EPSILON 0.09991008999999987 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.865736 / Loss  0.26559993624687195\n",
      "fps: 7.046609422014942\n",
      "TIMESTEP 192 \t STATE explore \n",
      " EPSILON 0.09990909099999987 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.006448 / Loss  1.057860016822815\n",
      "----------Random Action----------\n",
      "fps: 6.455546303721135\n",
      "TIMESTEP 193 \t STATE explore \n",
      " EPSILON 0.09990809199999987 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.063151 / Loss  0.083844855427742\n",
      "fps: 7.096576835095223\n",
      "TIMESTEP 194 \t STATE explore \n",
      " EPSILON 0.09990709299999986 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.863666 / Loss  0.10873600840568542\n",
      "fps: 6.4555761114822126\n",
      "TIMESTEP 195 \t STATE explore \n",
      " EPSILON 0.09990609399999986 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.043464 / Loss  0.21060553193092346\n",
      "----------Random Action----------\n",
      "fps: 7.147270124735874\n",
      "TIMESTEP 196 \t STATE explore \n",
      " EPSILON 0.09990509499999986 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  16.147255 / Loss  0.12818586826324463\n",
      "fps: 7.198655799097573\n",
      "TIMESTEP 197 \t STATE explore \n",
      " EPSILON 0.09990409599999986 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.831142 / Loss  0.13942638039588928\n",
      "fps: 6.373305171667135\n",
      "TIMESTEP 198 \t STATE explore \n",
      " EPSILON 0.09990309699999986 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.910165 / Loss  0.13911223411560059\n",
      "fps: 6.373363278169394\n",
      "TIMESTEP 199 \t STATE explore \n",
      " EPSILON 0.09990209799999986 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.152347 / Loss  0.2557971775531769\n",
      "fps: 7.198705219455348\n",
      "TIMESTEP 200 \t STATE explore \n",
      " EPSILON 0.09990109899999985 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.269814 / Loss  0.12865287065505981\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 201 \t STATE explore \n",
      " EPSILON 0.09990009999999985 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.605275 / Loss  0.41837018728256226\n",
      "fps: 7.046573906381778\n",
      "TIMESTEP 202 \t STATE explore \n",
      " EPSILON 0.09989910099999985 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.231386 / Loss  0.5382270216941833\n",
      "fps: 7.096576835095223\n",
      "TIMESTEP 203 \t STATE explore \n",
      " EPSILON 0.09989810199999985 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  8.643097 / Loss  0.08886587619781494\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 204 \t STATE explore \n",
      " EPSILON 0.09989710299999985 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.484361 / Loss  0.19817838072776794\n",
      "fps: 7.198668154123402\n",
      "TIMESTEP 205 \t STATE explore \n",
      " EPSILON 0.09989610399999985 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.195115 / Loss  0.2041603922843933\n",
      "fps: 7.096516800132648\n",
      "TIMESTEP 206 \t STATE explore \n",
      " EPSILON 0.09989510499999985 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  16.746046 / Loss  0.09663443267345428\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 207 \t STATE explore \n",
      " EPSILON 0.09989410599999984 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.00502 / Loss  0.5533849000930786\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 208 \t STATE explore \n",
      " EPSILON 0.09989310699999984 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.847027 / Loss  0.794259786605835\n",
      "fps: 7.195321492963884\n",
      "TIMESTEP 209 \t STATE explore \n",
      " EPSILON 0.09989210799999984 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.847842 / Loss  0.8096373081207275\n",
      "fps: 7.094332218687153\n",
      "TIMESTEP 210 \t STATE explore \n",
      " EPSILON 0.09989110899999984 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.605389 / Loss  0.33510950207710266\n",
      "fps: 6.411367812191416\n",
      "TIMESTEP 211 \t STATE explore \n",
      " EPSILON 0.09989010999999984 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.618649 / Loss  0.12672647833824158\n",
      "fps: 7.19635850160508\n",
      "TIMESTEP 212 \t STATE explore \n",
      " EPSILON 0.09988911099999984 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.157111 / Loss  0.8547347784042358\n",
      "fps: 6.372462746432652\n",
      "TIMESTEP 213 \t STATE explore \n",
      " EPSILON 0.09988811199999983 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.25661 / Loss  0.3368109464645386\n",
      "fps: 6.4556357278301775\n",
      "TIMESTEP 214 \t STATE explore \n",
      " EPSILON 0.09988711299999983 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  16.305452 / Loss  0.07502048462629318\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 215 \t STATE explore \n",
      " EPSILON 0.09988611399999983 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.82692 / Loss  0.07572642713785172\n",
      "fps: 6.414191555412825\n",
      "TIMESTEP 216 \t STATE explore \n",
      " EPSILON 0.09988511499999983 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.07027 / Loss  0.4056224226951599\n",
      "fps: 7.096492786432029\n",
      "TIMESTEP 217 \t STATE explore \n",
      " EPSILON 0.09988411599999983 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.401592 / Loss  0.7747740745544434\n",
      "fps: 7.096612856560338\n",
      "TIMESTEP 218 \t STATE explore \n",
      " EPSILON 0.09988311699999983 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.903874 / Loss  0.24970228970050812\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 219 \t STATE explore \n",
      " EPSILON 0.09988211799999983 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  14.101239 / Loss  0.637793242931366\n",
      "fps: 7.250835842878826\n",
      "TIMESTEP 220 \t STATE explore \n",
      " EPSILON 0.09988111899999982 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.256152 / Loss  0.37972456216812134\n",
      "----------Random Action----------\n",
      "fps: 7.041652536254999\n",
      "TIMESTEP 221 \t STATE explore \n",
      " EPSILON 0.09988011999999982 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  7.565928 / Loss  0.4459194540977478\n",
      "fps: 7.195309149423079\n",
      "TIMESTEP 222 \t STATE explore \n",
      " EPSILON 0.09987912099999982 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.726278 / Loss  0.062222920358181\n",
      "----------Random Action----------\n",
      "fps: 7.14316807368136\n",
      "TIMESTEP 223 \t STATE explore \n",
      " EPSILON 0.09987812199999982 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.545482 / Loss  0.6338781714439392\n",
      "fps: 7.143764221771062\n",
      "TIMESTEP 224 \t STATE explore \n",
      " EPSILON 0.09987712299999982 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.907683 / Loss  0.15166985988616943\n",
      "fps: 7.0442898194380765\n",
      "TIMESTEP 225 \t STATE explore \n",
      " EPSILON 0.09987612399999982 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.601122 / Loss  0.1992616504430771\n",
      "fps: 7.145139391668016\n",
      "TIMESTEP 226 \t STATE explore \n",
      " EPSILON 0.09987512499999981 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  7.2687793 / Loss  0.6226040720939636\n",
      "fps: 7.044810413604871\n",
      "TIMESTEP 227 \t STATE explore \n",
      " EPSILON 0.09987412599999981 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  2.513999 / Loss  0.9182776808738708\n",
      "----------Random Action----------\n",
      "fps: 7.147306662667828\n",
      "TIMESTEP 228 \t STATE explore \n",
      " EPSILON 0.09987312699999981 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  8.005632 / Loss  0.2995528280735016\n",
      "fps: 7.250860912498099\n",
      "TIMESTEP 229 \t STATE explore \n",
      " EPSILON 0.09987212799999981 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.395782 / Loss  0.787746787071228\n",
      "fps: 7.046597583430776\n",
      "TIMESTEP 230 \t STATE explore \n",
      " EPSILON 0.09987112899999981 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.577409 / Loss  0.19623062014579773\n",
      "fps: 7.19868050919164\n",
      "TIMESTEP 231 \t STATE explore \n",
      " EPSILON 0.09987012999999981 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  6.894338 / Loss  0.26157715916633606\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 232 \t STATE explore \n",
      " EPSILON 0.0998691309999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.5069685 / Loss  0.25971078872680664\n",
      "fps: 6.373334224785824\n",
      "TIMESTEP 233 \t STATE explore \n",
      " EPSILON 0.0998681319999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.530515 / Loss  0.16482223570346832\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 234 \t STATE explore \n",
      " EPSILON 0.0998671329999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.96721 / Loss  0.29192429780960083\n",
      "fps: 7.1472457663221105\n",
      "TIMESTEP 235 \t STATE explore \n",
      " EPSILON 0.0998661339999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.749116 / Loss  0.5528911352157593\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 236 \t STATE explore \n",
      " EPSILON 0.0998651349999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  7.556373 / Loss  0.11855302006006241\n",
      "fps: 7.198705219455348\n",
      "TIMESTEP 237 \t STATE explore \n",
      " EPSILON 0.0998641359999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.587358 / Loss  0.901857316493988\n",
      "----------Random Action----------\n",
      "fps: 7.121579334314729\n",
      "TIMESTEP 238 \t STATE explore \n",
      " EPSILON 0.0998631369999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.333147 / Loss  0.3271423578262329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 7.096528807043903\n",
      "TIMESTEP 239 \t STATE explore \n",
      " EPSILON 0.0998621379999998 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.986844 / Loss  0.35247042775154114\n",
      "fps: 7.250823308134198\n",
      "TIMESTEP 240 \t STATE explore \n",
      " EPSILON 0.0998611389999998 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  7.54773 / Loss  0.6968091726303101\n",
      "fps: 7.096528807043903\n",
      "TIMESTEP 241 \t STATE explore \n",
      " EPSILON 0.09986013999999979 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  7.581063 / Loss  0.8496133685112\n",
      "fps: 7.046656776749398\n",
      "TIMESTEP 242 \t STATE explore \n",
      " EPSILON 0.09985914099999979 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  7.7228413 / Loss  0.5732768177986145\n",
      "fps: 7.193199026570375\n",
      "TIMESTEP 243 \t STATE explore \n",
      " EPSILON 0.09985814199999979 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.203072 / Loss  0.1537107229232788\n",
      "fps: 7.194149045652264\n",
      "TIMESTEP 244 \t STATE explore \n",
      " EPSILON 0.09985714299999979 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.378609 / Loss  0.32946616411209106\n",
      "fps: 7.143058587909112\n",
      "TIMESTEP 245 \t STATE explore \n",
      " EPSILON 0.09985614399999979 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.812945 / Loss  0.34029537439346313\n",
      "fps: 7.092412818175823\n",
      "TIMESTEP 246 \t STATE explore \n",
      " EPSILON 0.09985514499999978 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.95374 / Loss  0.5653625726699829\n",
      "fps: 7.194260103223461\n",
      "TIMESTEP 247 \t STATE explore \n",
      " EPSILON 0.09985414599999978 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.6846 / Loss  0.41426438093185425\n",
      "fps: 7.094092236811171\n",
      "TIMESTEP 248 \t STATE explore \n",
      " EPSILON 0.09985314699999978 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.02181 / Loss  0.5031039118766785\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 249 \t STATE explore \n",
      " EPSILON 0.09985214799999978 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.367439 / Loss  0.10294055193662643\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 250 \t STATE explore \n",
      " EPSILON 0.09985114899999978 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  12.35826 / Loss  0.11514927446842194\n",
      "fps: 7.198705219455348\n",
      "TIMESTEP 251 \t STATE explore \n",
      " EPSILON 0.09985014999999978 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.296014 / Loss  0.36551132798194885\n",
      "fps: 7.198668154123402\n",
      "TIMESTEP 252 \t STATE explore \n",
      " EPSILON 0.09984915099999978 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  13.058657 / Loss  0.1705053299665451\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 253 \t STATE explore \n",
      " EPSILON 0.09984815199999977 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  9.25115 / Loss  0.7517869472503662\n",
      "fps: 7.147270124735874\n",
      "TIMESTEP 254 \t STATE explore \n",
      " EPSILON 0.09984715299999977 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.123739 / Loss  0.11409832537174225\n",
      "fps: 7.096528807043903\n",
      "TIMESTEP 255 \t STATE explore \n",
      " EPSILON 0.09984615399999977 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  10.807267 / Loss  0.37167495489120483\n",
      "fps: 7.198655799097573\n",
      "TIMESTEP 256 \t STATE explore \n",
      " EPSILON 0.09984515499999977 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.307276 / Loss  0.3306129276752472\n",
      "fps: 6.997294041887297\n",
      "TIMESTEP 257 \t STATE explore \n",
      " EPSILON 0.09984415599999977 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  13.281514 / Loss  0.24798254668712616\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 258 \t STATE explore \n",
      " EPSILON 0.09984315699999977 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.422393 / Loss  0.1584922969341278\n",
      "fps: 7.198668154123402\n",
      "TIMESTEP 259 \t STATE explore \n",
      " EPSILON 0.09984215799999976 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  7.8612957 / Loss  0.3180602192878723\n",
      "fps: 7.198643444114154\n",
      "TIMESTEP 260 \t STATE explore \n",
      " EPSILON 0.09984115899999976 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  12.579919 / Loss  0.239959716796875\n",
      "fps: 7.096576835095223\n",
      "TIMESTEP 261 \t STATE explore \n",
      " EPSILON 0.09984015999999976 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  18.489697 / Loss  0.1683386266231537\n",
      "fps: 7.198631089173144\n",
      "TIMESTEP 262 \t STATE explore \n",
      " EPSILON 0.09983916099999976 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  10.958639 / Loss  0.40392041206359863\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 263 \t STATE explore \n",
      " EPSILON 0.09983816199999976 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  11.475293 / Loss  0.17279177904129028\n",
      "fps: 7.145005502311652\n",
      "TIMESTEP 264 \t STATE explore \n",
      " EPSILON 0.09983716299999976 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.376977 / Loss  0.20669278502464294\n",
      "fps: 7.14316807368136\n",
      "TIMESTEP 265 \t STATE explore \n",
      " EPSILON 0.09983616399999976 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.888237 / Loss  0.0961768627166748\n",
      "fps: 7.094512215749555\n",
      "TIMESTEP 266 \t STATE explore \n",
      " EPSILON 0.09983516499999975 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  17.656181 / Loss  0.9924235343933105\n",
      "----------Random Action----------\n",
      "fps: 6.452298907157625\n",
      "TIMESTEP 267 \t STATE explore \n",
      " EPSILON 0.09983416599999975 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.950036 / Loss  0.2630459666252136\n",
      "fps: 7.046514714455396\n",
      "TIMESTEP 268 \t STATE explore \n",
      " EPSILON 0.09983316699999975 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.157385 / Loss  0.7997782826423645\n",
      "fps: 7.198742285168988\n",
      "TIMESTEP 269 \t STATE explore \n",
      " EPSILON 0.09983216799999975 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.880146 / Loss  0.3539639413356781\n",
      "fps: 7.19868050919164\n",
      "TIMESTEP 270 \t STATE explore \n",
      " EPSILON 0.09983116899999975 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  16.8646 / Loss  0.15838788449764252\n",
      "fps: 7.046562067916946\n",
      "TIMESTEP 271 \t STATE explore \n",
      " EPSILON 0.09983016999999975 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.277596 / Loss  1.256738305091858\n",
      "fps: 7.096588842209631\n",
      "TIMESTEP 272 \t STATE explore \n",
      " EPSILON 0.09982917099999974 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.939458 / Loss  2.0369653701782227\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 273 \t STATE explore \n",
      " EPSILON 0.09982817199999974 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.18111 / Loss  0.5638401508331299\n",
      "fps: 7.2508483776667925\n",
      "TIMESTEP 274 \t STATE explore \n",
      " EPSILON 0.09982717299999974 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  20.368013 / Loss  2.365760326385498\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 275 \t STATE explore \n",
      " EPSILON 0.09982617399999974 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  21.520264 / Loss  0.21173112094402313\n",
      "fps: 7.096564828021447\n",
      "TIMESTEP 276 \t STATE explore \n",
      " EPSILON 0.09982517499999974 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.212719 / Loss  0.2714032530784607\n",
      "fps: 7.197766348676816\n",
      "TIMESTEP 277 \t STATE explore \n",
      " EPSILON 0.09982417599999974 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  18.495768 / Loss  1.145021915435791\n",
      "fps: 7.092412818175823\n",
      "TIMESTEP 278 \t STATE explore \n",
      " EPSILON 0.09982317699999974 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  18.276684 / Loss  0.4853571057319641\n",
      "fps: 7.195346180172545\n",
      "TIMESTEP 279 \t STATE explore \n",
      " EPSILON 0.09982217799999973 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.750233 / Loss  0.14992617070674896\n",
      "fps: 7.141732859976672\n",
      "TIMESTEP 280 \t STATE explore \n",
      " EPSILON 0.09982117899999973 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  18.926958 / Loss  1.6557741165161133\n",
      "fps: 6.371901253323205\n",
      "TIMESTEP 281 \t STATE explore \n",
      " EPSILON 0.09982017999999973 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  17.165932 / Loss  0.6281882524490356\n",
      "fps: 7.248630395033821\n",
      "TIMESTEP 282 \t STATE explore \n",
      " EPSILON 0.09981918099999973 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  19.622019 / Loss  0.4171086847782135\n",
      "fps: 7.146247214290096\n",
      "TIMESTEP 283 \t STATE explore \n",
      " EPSILON 0.09981818199999973 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  18.827179 / Loss  1.1943366527557373\n",
      "fps: 7.09531631415391\n",
      "TIMESTEP 284 \t STATE explore \n",
      " EPSILON 0.09981718299999973 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  19.308601 / Loss  0.21941570937633514\n",
      "fps: 7.198692864302289\n",
      "TIMESTEP 285 \t STATE explore \n",
      " EPSILON 0.09981618399999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  17.737661 / Loss  0.4617012143135071\n",
      "fps: 7.046585744886388\n",
      "TIMESTEP 286 \t STATE explore \n",
      " EPSILON 0.09981518499999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  19.020863 / Loss  0.37958866357803345\n",
      "fps: 6.373295487353102\n",
      "TIMESTEP 287 \t STATE explore \n",
      " EPSILON 0.09981418599999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  20.037859 / Loss  0.6151062250137329\n",
      "fps: 7.198742285168988\n",
      "TIMESTEP 288 \t STATE explore \n",
      " EPSILON 0.09981318699999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  21.184273 / Loss  1.407848834991455\n",
      "----------Random Action----------\n",
      "fps: 6.853471503945281\n",
      "TIMESTEP 289 \t STATE explore \n",
      " EPSILON 0.09981218799999972 \t ACTION 1 \n",
      " REWARD -1 / Q_MAX  20.27116 / Loss  1.1030426025390625\n",
      "fps: 6.6708081844279565\n",
      "TIMESTEP 290 \t STATE explore \n",
      " EPSILON 0.09981118899999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  21.240316 / Loss  0.7517845034599304\n",
      "fps: 7.147257945508239\n",
      "TIMESTEP 291 \t STATE explore \n",
      " EPSILON 0.09981018999999972 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  18.774006 / Loss  0.22249358892440796\n",
      "fps: 7.147221408074378\n",
      "TIMESTEP 292 \t STATE explore \n",
      " EPSILON 0.09980919099999971 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  19.077978 / Loss  0.29834622144699097\n",
      "fps: 7.246701728439429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 293 \t STATE explore \n",
      " EPSILON 0.09980819199999971 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  20.657885 / Loss  0.413770467042923\n",
      "----------Random Action----------\n",
      "fps: 7.04359187073242\n",
      "TIMESTEP 294 \t STATE explore \n",
      " EPSILON 0.09980719299999971 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  14.7382345 / Loss  0.20387002825737\n",
      "fps: 7.090698232694642\n",
      "TIMESTEP 295 \t STATE explore \n",
      " EPSILON 0.09980619399999971 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.98555 / Loss  0.3135032057762146\n",
      "fps: 7.197877517959068\n",
      "TIMESTEP 296 \t STATE explore \n",
      " EPSILON 0.09980519499999971 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  19.956825 / Loss  0.34968101978302\n",
      "fps: 7.145175907817898\n",
      "TIMESTEP 297 \t STATE explore \n",
      " EPSILON 0.0998041959999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  11.179217 / Loss  0.6655891537666321\n",
      "fps: 7.1450176738639755\n",
      "TIMESTEP 298 \t STATE explore \n",
      " EPSILON 0.0998031969999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  15.838426 / Loss  0.8208165168762207\n",
      "fps: 7.094812231151648\n",
      "TIMESTEP 299 \t STATE explore \n",
      " EPSILON 0.0998021979999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  15.836218 / Loss  0.23268315196037292\n",
      "fps: 6.372501473743064\n",
      "TIMESTEP 300 \t STATE explore \n",
      " EPSILON 0.0998011989999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.3095 / Loss  0.5442667007446289\n",
      "fps: 7.147233587177491\n",
      "TIMESTEP 301 \t STATE explore \n",
      " EPSILON 0.0998001999999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  14.947207 / Loss  0.2753802537918091\n",
      "fps: 6.455595983475857\n",
      "TIMESTEP 302 \t STATE explore \n",
      " EPSILON 0.0997992009999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  19.22027 / Loss  0.1951703280210495\n",
      "fps: 6.455595983475857\n",
      "TIMESTEP 303 \t STATE explore \n",
      " EPSILON 0.0997982019999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  17.698442 / Loss  0.3730766773223877\n",
      "fps: 6.373334224785824\n",
      "TIMESTEP 304 \t STATE explore \n",
      " EPSILON 0.0997972029999997 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  16.552322 / Loss  0.560279905796051\n",
      "fps: 7.096540813995787\n",
      "TIMESTEP 305 \t STATE explore \n",
      " EPSILON 0.0997962039999997 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  17.001493 / Loss  0.3219589591026306\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 306 \t STATE explore \n",
      " EPSILON 0.09979520499999969 \t ACTION 0 \n",
      " REWARD 0.1 / Q_MAX  16.941824 / Loss  0.2256588339805603\n",
      "fps: 6.215017499770325\n",
      "TIMESTEP 307 \t STATE explore \n",
      " EPSILON 0.09979420599999969 \t ACTION 1 \n",
      " REWARD 0.1 / Q_MAX  18.920708 / Loss  0.27950215339660645\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=74.0.3729.169)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Windows NT 10.0.17134 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4829a0a02061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-12216cee503d>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-7c448d416f82>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fps: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# helpful for measuring frame rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-08acf199be89>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mactions_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# storing actions in a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mis_over\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;31m#game over\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fa05428ae9c5>\u001b[0m in \u001b[0;36mget_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mARROW_DOWN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return Runner.instance_.distanceMeter.digits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_array\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the javascript object is of type array with score in the formate[1,0,0] which is 100.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    634\u001b[0m         return self.execute(command, {\n\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=74.0.3729.169)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Windows NT 10.0.17134 x86_64)\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
